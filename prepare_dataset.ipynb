{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MhxiaBRX-ku_"
      },
      "source": [
        "# Pre-Process Speech Dataset for *Efficient Speech* Model Training\n",
        "## Important: This notebook requires a GPU with CUDA available!\n",
        "\n",
        "  The notebook demonstrates preprocessing a speech dataset into the format expected by EfficientSpeech for training checkpoints.\n",
        "\n",
        "### **Dataset Specifications**\n",
        "#### Input Dataset format:\n",
        "This notebook assumes your dataset is a **folder** of Mono 22050Hz .wav files, with each audio file having a transcription text file with the same name.\n",
        "\n",
        "* `MyDataset`:  folder\n",
        "  - `speaker_001.wav`: an audio file\n",
        "  - `speaker_001.txt`: text transcription of speaker_001.wav\n",
        "  - ...\n",
        "  - `speaker_999.wav`\n",
        "  - `speaker_999.txt`\n",
        "\n",
        "#### Output Dataset format:\n",
        "The output for training is in this format:\n",
        "* `content/output_dataset`:  folder\n",
        "  - `configs/MyDataset`: folder\n",
        "    - `preprocess.yaml`: Only this file is necessary to train EfficientSpeech models\n",
        "    - `model.yaml`\n",
        "    - `train.yaml`\n",
        "  - `preprocessed_data/MyDataset`: folder\n",
        "    - `duration`: folder\n",
        "    - `energy`: folder\n",
        "    - `mel`: folder\n",
        "    - `pitch`: folder\n",
        "    - `TextGrid/universal`: folder of .TextGrid files\n",
        "    - `speakers.json`\n",
        "    - `stats.json`\n",
        "    - `train.txt`\n",
        "    - `val.txt`     \n",
        "  - `raw_data/universal`: folder\n",
        "    - `speaker_001.wav`\n",
        "    - `speaker_001.txt`\n",
        "    - ...\n",
        "    - `speaker_999.wav`\n",
        "    - `speaker_999.txt`\n",
        "\n",
        "### Links\n",
        "EfficientSpeech repository: https://github.com/roatienza/efficientspeech  \n",
        "FastSpeech2 repository: https://github.com/ming024/FastSpeech2  \n",
        "Montreal Forced Aligner Tutorial: https://eleanorchodroff.com/mfa_tutorial.html"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob9m3Aiu44Nt"
      },
      "source": [
        "### Configuration Settings\n",
        "##### Dataset\n",
        "* dataset_name: The name of your dataset\n",
        "* dataset_path: A directory with the raw audio files + text transcriptions. The text and audio file names should match.\n",
        "* speaker_name: One of 'universal', 'LJSpeech'\n",
        "* val_size: The size of your validation set. (default: 512)  \n",
        "  - 0 < *val_size* < total audio files. \n",
        "  - Example: For FastSpeech2, LJSpeech config has 13,100 audio files with a *val_size* of 512.\n",
        "\n",
        "##### Output\n",
        "* output_path: Where to save the working files\n",
        "* output_zip_path: Where to save the finished dataset as a .zip file\n",
        "\n",
        "##### MFA (Montreal Forced Aligner) Settings\n",
        "* text_file_extension: the file format extension of the text transcription files.\n",
        "* corpus_name: 'metadata.csv'\n",
        "* lexicon_path: the lexicon/dictionary to use when running MFA.\n",
        "* dictionary_file: the lexicon/dictionary to use when preprocessing dataset  \n",
        "* allow_overwrite_existing_corpus: Enable to allow overwriting existing `corpus_name` file.\n",
        "* acoustic_model: MFA acoustic model (default: 'english_us_arpa')\n",
        "* dictionary_model - MFA dictionary model (unused)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrU71RMluboT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "from yaml import CLoader as Loader, CDumper as Dumper\n",
        "import librosa\n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from preprocessor import Preprocessor\n",
        "from tqdm.auto import tqdm\n",
        "import shutil\n",
        "from utils import get_yaml_path, get_yaml_contents, write_yaml\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def create_data_df(transcript_path, data_path, speaker_id=None):\n",
        "    data_df = pd.read_csv(transcript_path, sep=\"|\")\n",
        "    data_df[\"wav_filename\"] = data_df[\"wav_filename\"].apply(\n",
        "        lambda x: os.path.join(data_path, x)\n",
        "    )\n",
        "    if speaker_id:\n",
        "        return data_df[data_df[\"client_id\"] == speaker_id].reset_index(drop=True)\n",
        "    return data_df\n",
        "\n",
        "\n",
        "def create_dataset(transcript_path, data_path, speaker_id, out_path):\n",
        "    data_df = create_data_df(\n",
        "        transcript_path=transcript_path, data_path=data_path, speaker_id=speaker_id\n",
        "    )\n",
        "    data_df = data_df[:100]\n",
        "    if os.path.isdir(out_path):\n",
        "        shutil.rmtree(out_path)\n",
        "    dst = os.path.join(out_path)\n",
        "    os.makedirs(dst)\n",
        "    for idx, row in tqdm(data_df.iterrows()):\n",
        "        audio_source = str(row[\"wav_filename\"])\n",
        "        audio_dst = os.path.join(dst, audio_source.split(\"/\")[-1])\n",
        "        shutil.copy(audio_source, audio_dst)\n",
        "        transcript_dst = audio_dst.replace(\"wav\", \"lab\")\n",
        "        with open(transcript_dst, \"w\") as f:\n",
        "            f.write(row[\"transcript\"])\n",
        "    data_df.to_csv(os.path.join(dst, \"metadata.csv\"), index=False)\n",
        "\n",
        "\n",
        "def prepare_align(config, data_path=None):\n",
        "    sampling_rate = config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n",
        "    max_wav_value = config[\"preprocessing\"][\"audio\"][\"max_wav_value\"]\n",
        "    if data_path is None:\n",
        "        data_path = config[\"path\"][\"corpus_path\"]\n",
        "    for filename in tqdm(os.listdir(data_path)):\n",
        "        if filename.split(\".\")[-1] == \"wav\":\n",
        "            wav_path = os.path.join(data_path, filename)\n",
        "            wav, sr = librosa.load(wav_path, sr=sampling_rate)\n",
        "            wav = wav / max(abs(wav)) * max_wav_value\n",
        "            wavfile.write(wav_path, sampling_rate, wav.astype(np.int16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_NAME = \"CML_Polish\"\n",
        "DATASET_ROOT = \"../cml-tts/\"\n",
        "TRAIN_PATH = DATASET_ROOT\n",
        "TRAIN_TRANSCRIPT = os.path.join(DATASET_ROOT, \"train.csv\")\n",
        "SPEAKER_ID = 6892  ##10K+ samples for this ID, best to train TTS\n",
        "CONFIG_DIR = os.path.join(\"./config\", DATASET_NAME)\n",
        "OUTPUT_PATH = \"../output_dataset\"\n",
        "\n",
        "ACOUSTIC_MODEL = \"polish_mfa\"\n",
        "DICTIONARY_MODEL = \"polish_mfa\"\n",
        "SPEAKER_NAME = \"universal\"\n",
        "\n",
        "PREPROCESSED_DATA_PATH = os.path.join(OUTPUT_PATH, \"preprocessed_data\", DATASET_NAME)\n",
        "RAW_PATH = os.path.join(OUTPUT_PATH, \"raw_data\")\n",
        "RAW_DATA_SPEAKER_PATH = os.path.join(RAW_PATH, SPEAKER_NAME)\n",
        "CORPUS_PATH = RAW_DATA_SPEAKER_PATH\n",
        "TEXTGRID_DIR = os.path.join(PREPROCESSED_DATA_PATH, \"TextGrid\", SPEAKER_NAME)\n",
        "CONFIG = get_yaml_contents(CONFIG_DIR, \"preprocess\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create directory structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "os.makedirs(CORPUS_PATH, exist_ok=True)\n",
        "os.makedirs(PREPROCESSED_DATA_PATH, exist_ok=True)\n",
        "os.makedirs(RAW_DATA_SPEAKER_PATH, exist_ok=True)\n",
        "os.makedirs(TEXTGRID_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create dataset and apply wav preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#create dataset and apply wav preprocessing\n",
        "create_dataset(TRAIN_TRANSCRIPT, TRAIN_PATH, SPEAKER_ID, RAW_DATA_SPEAKER_PATH)\n",
        "prepare_align(CONFIG, data_path=RAW_DATA_SPEAKER_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run MFA forced alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "align_cmd_opts = f\"{CORPUS_PATH} {DICTIONARY_MODEL} {ACOUSTIC_MODEL} {TEXTGRID_DIR}\"\n",
        "command = f\"mfa align --clean --single_speaker {align_cmd_opts}\"\n",
        "print(command)\n",
        "with open(\"test.log\", \"wb\") as f:\n",
        "    process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n",
        "    for c in iter(lambda: process.stdout.read(1), b\"\"):\n",
        "        sys.stdout.buffer.write(c)\n",
        "        f.buffer.write(c)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kHqaO0dcCofH"
      },
      "source": [
        "# Build dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrR33GuhDArV",
        "outputId": "4474a70a-5506-45c6-f32d-31213037dec4"
      },
      "outputs": [],
      "source": [
        "\n",
        "preprocessor = Preprocessor(\n",
        "    CONFIG, raw_path=RAW_PATH, preprocessed_path=PREPROCESSED_DATA_PATH\n",
        ")\n",
        "preprocessor.build_from_path()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
