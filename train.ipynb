{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liZc2svqHrM0"
      },
      "source": [
        "# Configuration options\n",
        "#### Make sure to configure the settings in the `Configuration Settings` section below before running further cells.\n",
        "\n",
        "##### Dataset\n",
        "* dataset_name: name of the dataset (default 'MyDataset')\n",
        "* dataset_location: absolute path to the prepared dataset folder (default: '/content/output_dataset')\n",
        "* speaker_name: name of the speaker in raw_data folder (default: 'universal')\n",
        "* config_dir: absolute path to EfficientSpeech configuration directory\n",
        "* lexicon_path: absolute path to a .txt file with the lexicon/dictionary the dataset is prepared for (defaults to `librispeech-lexicon.txt`)\n",
        "\n",
        "##### Output\n",
        "* output_dir: A path to save all generated .ckpt files + logs to. A folder with your dataset name will be created in this folder.\n",
        "* infer_device: Device used for inference after training. One of 'cuda', 'cpu' (default: 'cuda')\n",
        "\n",
        "##### Model training options\n",
        "* accelerator: One of `cpu`, `gpu`\n",
        "* devices: Will be mapped to either `gpus`, `tpu_cores`, `num_processes` or `ipus`, based on the accelerator type, per pytorch-lightning documentation.\n",
        "* batch_size: (default: 128)\n",
        "* num_workers: (default: 4)\n",
        "* precision: (default: 16-mixed)\n",
        "* model_size_to_train: One of 'tiny', 'small', 'base' (default: tiny)\n",
        "* max_epochs: The number of epochs to stop training at (default: 5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install tensorboard\n",
        "# !pip install -r requirements.txt\n",
        "# !wget --continue -nv -O /home/atlas/code/l2i/experiment/checkpoints/tiny_eng_266k.ckpt  https://github.com/roatienza/efficientspeech/releases/download/pytorch2.0.1/tiny_eng_266k.ckpt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IJwbFktvjXL",
        "outputId": "fd952f21-fcbb-4ba2-dc2b-5ab3691a678f"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_name = 'CML_Polish' \n",
        "dataset_location = '../output_dataset' \n",
        "speaker_name = 'universal' \n",
        "config_dir = './configs/CML_Polish' \n",
        "output_dir = '../checkpoints' \n",
        "cmd_line_opts = \"\"\n",
        "!mkdir /home/atlas/code/l2i/new_experiment/checkpoints\n",
        "\n",
        "\n",
        "# cmd_line_opts = \"--accelerator gpu --devices 1 --num_workers 4 --precision 16-mixed --batch-size 8 --head 1 --reduction 4 --expansion 1 --kernel-size 3 --n-blocks 2 --block-depth 2 --max_epochs 100 --infer-device cuda\"\n",
        "pp_config_path = os.path.join(\"/home/atlas/code/l2i/new_experiment/output_dataset/configs/CML_Polish/\", 'preprocess.yaml')\n",
        "pp_config_arg = f'--preprocess-config {pp_config_path}'\n",
        "training_opts = ' '.join([pp_config_arg, cmd_line_opts])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaDwDvFNkJ3c"
      },
      "source": [
        "### Launch TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rglH-KDOr5E"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir ../lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0-1pUyp6c01"
      },
      "source": [
        "### Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(f'Running training with arguments: {training_opts}')\n",
        "\n",
        "%cd /content/efficientspeech/\n",
        "!python /content/efficientspeech/train.py $training_opts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JayYMPZAHQa9"
      },
      "source": [
        "# Run inference on latest trained checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAspCqnQHP5X"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Audio, display\n",
        "import os\n",
        "\n",
        "sentence = 'The quick brown fox jumped over the lazy dog.' #@param {type:'string'}\n",
        "\n",
        "%cd /content/efficientspeech/\n",
        "\n",
        "# Get latest run checkpoint\n",
        "latest_run_folder = !ls -td -- lightning_logs/* | head -n 1\n",
        "latest_run_folder = latest_run_folder[0]\n",
        "latest_run_name = os.path.basename(latest_run_folder)\n",
        "ckpt_folder = os.path.join(latest_run_folder, 'checkpoints')\n",
        "latest_ckpt = !ls -td -- $ckpt_folder\\/* | head -n 1\n",
        "latest_ckpt = os.path.abspath(latest_ckpt[0])\n",
        "latest_ckpt_name = os.path.basename(latest_ckpt)\n",
        "# Output wav \n",
        "output_wav_name = latest_run_name + '.wav'\n",
        "\n",
        "print(f'Found checkpoint \"{latest_ckpt}')\n",
        "\n",
        "# Run inference with latest checkpoint\n",
        "inference_args = f'--checkpoint {latest_ckpt} {model_opts} ' \\\n",
        "  f'--infer-device {infer_device} --text \"{sentence}\" ' \\\n",
        "  f'--wav-filename {output_wav_name}'\n",
        "print(f'Running inference with arguments: {inference_args}')\n",
        "!python demo.py $inference_args\n",
        "\n",
        "# Display inference result\n",
        "output_wav_path = os.path.join('/content/efficientspeech/outputs', output_wav_name)\n",
        "print(f'\\nInference result: {output_wav_path}')\n",
        "display(Audio(os.path.abspath(output_wav_path)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
